<!--
 * @Author: Xuan 917125317@qq.com
 * @Date: 2022-05-22 18:40:12
 * @LastEditors: Xuan 917125317@qq.com
 * @LastEditTime: 2022-05-23 20:39:45
 * @FilePath: \tcyily.github.io\_posts\2022-05-22-Water.md
-->
# 水

## 折射
> 水体为透明，可以直接看到水底的效果。可直接采用透明度混合来实现。
> 但是在需要对折射效果进行波动效果时不好处理。
> 因此，采用另外一种方案。
> 将MainCamera看见的界面写入Rt里，传递给水体Shader进行处理。

此时获得的Rt是在摄像机坐标下的，因此需要获得水体各个像素在屏幕空间对应的uv偏移量来采样。
下面是业务代码：

    FragData Vert (appdata v){
        FragData output;
        VertexPositionInputs vertexInput = GetVertexPositionInputs(v.pos.xyz);
        output.vertex = vertexInput.positionCS;
        output.posVs = ComputeScreenPos(output.vertex);
    }

    float4 Frag (FragData i) : SV_Target
    {
        float3 normalWs = normalize(i.posWs + float3(0, 1, 0));
        // 采样偏移
        float2 screenUv = i.posVs.xy / i.posVs.w;
    }

URP中对应的接口代码如下：

    // 将ObjectSpace下的数据转化到对应的空间下
    VertexPositionInputs GetVertexPositionInputs(float3 positionOS)
    {
        VertexPositionInputs input;
        input.positionWS = TransformObjectToWorld(positionOS);
        input.positionVS = TransformWorldToView(input.positionWS);
        input.positionCS = TransformWorldToHClip(input.positionWS);

        float4 ndc = input.positionCS * 0.5f;
        input.positionNDC.xy = float2(ndc.x, ndc.y * _ProjectionParams.x) + ndc.w;
        input.positionNDC.zw = input.positionCS.zw;

        return input;
    }

    // -w ~ w 映射到  之间。后续 / w 转到 0 ~ 1 之下
    float4 ComputeScreenPos(float4 positionCS)
    {
        float4 o = positionCS * 0.5f;
        o.xy = float2(o.x, o.y0 ~ w * _ProjectionParams.x) + o.w;
        o.zw = positionCS.zw;
        return o;
    }

> 为什么screenUv在Frag才进行/w操作呢？
> 中心坐标插值采用的是线性插值，但是投影空间不是线性空间。如果在Vert计算出在有流水线内部进行插值会有偏差

----

## 反射实现
### 方案1.静态采样
> 用静态的相机先渲染好一张CubeMap。效果较差，也可以用Box Projection Cube Map来修复采样的角度达到较好的效果。

### 方案2.实时才牙膏
> 在水面之下实例化一个相机，相当于水面之下有一个相机观察水面之上的游戏世界，再将其渲染到水面之中。缺点是DC会翻倍

算法思路：
    水底相机如何计算出来，有哪一些坑。
    

代码：
    
    {
        GL.invertCulling = true;
        //反射矩阵
        Matrix4x4 mir_matrix = GetMirrorMatrix();
        __camera_reflect_.worldToCameraMatrix = Camera.main.worldToCameraMatrix * mir_matrix;
        __camera_reflect_.transform.position = mir_matrix.MultiplyPoint(Camera.main.transform.position);
        __camera_reflect_.transform.forward = mir_matrix.MultiplyVector(Camera.main.transform.forward);
        //斜裁剪
        Vector4 clip_plane = GameHelper.GetPlaneInCameraSpace(__camera_reflect_, transform.position, transform.up);
        //__camera_reflect_.projectionMatrix = __camera_reflect_.CalculateObliqueMatrix(clip_plane);
        Matrix4x4 matrix4X4 = __camera_reflect_.projectionMatrix;
        __camera_reflect_.projectionMatrix = GameHelper.GetProjectionMatrixInPlane(matrix4X4, clip_plane);
        //渲染
        // __camera_reflect_.Render();
        UniversalRenderPipeline.RenderSingleCamera(context, __camera_reflect_); // render planar reflections
        water_effect_material_.SetTexture("_ReflectTex", __camera_reflect_texture_);
        GL.invertCulling = false;
    }
    
    // 反射矩阵函数
    private Matrix4x4 GetMirrorMatrix()
    {
        Vector3 normal = transform.up.normalized;
        float dis = Vector3.Dot(-normal, transform.position);//平面上所有的点都由过原点的平行面平移 n·p 而来， 故而 d 为 -n·p
        Matrix4x4 martix = new Matrix4x4();

        martix.m00 = 1 - 2 * normal.x * normal.x;
        martix.m01 = -2 * normal.x * normal.y;
        martix.m02 = -2 * normal.x * normal.z;
        martix.m03 = -2 * dis * normal.x;

        martix.m10 = -2 * normal.x * normal.y;
        martix.m11 = 1 - 2 * normal.y * normal.y;
        martix.m12 = -2 * normal.y * normal.z;
        martix.m13 = -2 * dis * normal.y;

        martix.m20 = -2 * normal.x * normal.z;
        martix.m21 = -2 * normal.y * normal.z;
        martix.m22 = 1 - 2 * normal.z * normal.z;
        martix.m23 = -2 * dis * normal.z;

        martix.m30 = 0;
        martix.m31 = 0;
        martix.m32 = 0;
        martix.m33 = 1;

        return martix;
    }

    // 斜裁剪函数,传入相机的裁剪矩阵 以及 改变后的近平面。
    // 推导过程大致简述为由于NDC各个方向上的范围都是在标准大小之内（-1~1），由此可以反推裁剪矩阵之中各列对应的是裁剪矩阵6个面的哪一个。【记得好像是裁剪矩阵1,2,3,4列分别为a,b,c,d;(a±d),(b±d),(c±d)这6个组合刚好是描述视锥体的6个面（具体可以自行网上查找一下推导过程）】
    public static Matrix4x4 GetProjectionMatrixInPlane(Matrix4x4 projection, Vector4 clip_plane)
    {
        Vector4 q = projection.inverse * new Vector4(
            sign(clip_plane.x),
            sign(clip_plane.y),
            1.0f,
            1.0f
        );
        Vector4 c = clip_plane * (2.0F / (Vector4.Dot(clip_plane, q)));

        projection[2] = c.x - projection[3];
        projection[6] = c.y - projection[7];
        projection[10] = c.z - projection[11];
        projection[14] = c.w - projection[15];
        return projection;
    }

> ?: invertCulling作用
>> 进行反射计算之后，对于水面相机而言，顶点的上下顺序发生了改变。而在渲染时，顶点间描述顺序需要为顺时针，对于相机而言才是可见的。否则会被裁剪掉。因此，需要在渲染前将裁剪逆时针反转一下。之后再进行

> ?: 为什么需要使用斜裁剪矩阵
>> 不修改视锥体


## 对反射和折射进行进行处理
### 1.模拟水波导致的成像波动
直接使用一张贴图表示某一时刻的水面采样波动效果，再通过伪随机的采样偏移来对这张图进行xy方向之上的扰动

    {
        float3 uvOffset0 = SAMPLE_TEXTURE2D(_NormalTex, sampler_NormalTex, i.uv.xy) * 2 - 1;
        float3 uvOffset1 = SAMPLE_TEXTURE2D(_NormalTex, sampler_NormalTex, i.uv.zw) * 2 - 1;
        uvOffset0 = 0.3 * uvOffset0;
        uvOffset1 = 0.3 * uvOffset1;
        float3 uvOffset = normalize(uvOffset0 + 0.5f * uvOffset1);
    }
### 2.菲涅尔效应
对于水体而言，当我们视线与水体法线的焦点越大，也就是我们看水面远处的时候，会发现水面之下的物体会看不清，水成了一种不透明的物体。而当你看脚下的水面时，则是透明的感觉。因此，通过dot(normal, viewDir)就可以获取对应的参数了

### 3.水体深度对反射折射的影响
当水过于深的时候，会看不清楚水底的东西（折射）。因此。需要根据水体深度对于折射的效果进行插值，那么如何得到水面距离水底的深度呢？
> 采用深度贴图，由于我们水体的Queue为Transparent。因此深度贴图记录的不会是相机到水面的深度。因此可以得到A：相机到此时渲染的像素方向上延伸到水底的深度，再在Frag着色器之中获得B：世界坐标系的相机位置 - 世界坐标系的像素点。 最后A - B，可以得出视线方向下的水体深度。然后对这个深度进行你所需要的缩小，最后映射到CS层传入的梯度颜色之中（Gradient Color，Unity的一个表示渐变色类型，编辑器之中上测表示透明度渐变，下边RGB渐变。）

> 有一个需要注意的点，我在CS层传入的为(4, 256)的数组，把它画在一个正方形上，那么下标为[1]的采用v需要在0.375才可以得到原始的颜色

## 水体与光作用

### 1.SSS 和 BRDF
水体对于光照的处理，Unity内置函数以及组件可以实现。暂时接入了一半，就不细讲了。

## 水体运动
在自然界之中，水面常常会有波浪出现。在水面之中只需要预设好一个波浪函数，再将其带入就可以实现了。但是简单的Sin波会过于单调。因此，我们引入多个sin波进行叠加，以此来模拟有波峰的水波纹。

代码 

    struct WaveData{
        float3 vertex;
        float3 normal;
    };
    
    //W(x,y,z) = A * sin(dot(d, xy) * w + t&speed)
    WaveData WaveSinOffset(float2 xy, float amplitude, float direction, float len, float speed){
        const float pi = 3.1415926;
        WaveData wave;
        UNITY_INITIALIZE_OUTPUT(WaveData, wave);
        wave.vertex = float3(.0f, .0f, .0f);
        float L = len;

        float W = 2*pi / L;
        float2 D = normalize(float2(sin(direction*pi), cos(direction*pi)));
        float A = amplitude;
        float WA = W * A;
        //float Q = 1/(WA);
        float Q = _WaveQ;
        float S = speed/W*_Time.y;

        float cosVal = cos(W*dot(D,xy)+S);
        float sinVal = sin(W*dot(D,xy)+S);

        wave.vertex.xz = Q * A * D.xy * cosVal;
        wave.vertex.y = (A * sinVal);

        wave.normal.xy = -D*xy*WA*cosVal;
        wave.normal.z = -Q*WA*sinVal;
        return wave;
    }

    WaveData WaveGerstner(float3 vertex){
        uint waveNum = 1;
        WaveData waveTotal;
        UNITY_INITIALIZE_OUTPUT(WaveData, waveTotal);
        for(uint  i = 0; i < waveNum; i++){
            WaveData wave = WaveSinOffset(vertex.xz, _WaveAmplitude, _WaveDirection, _WaveLen, _WaveSpeed);
            waveTotal.vertex += wave.vertex;
            waveTotal.normal += wave.normal;
        }

        waveTotal.normal.z = 1 - waveTotal.normal.z;
        return waveTotal;
    }

    v2f vert(a2v v){
        v2f o;

        //水波效果施工中
        WaveData waveData = WaveGerstner(v.vertex);
        v.vertex.xyz += waveData.vertex;
        //v.vertex.y = waveData.vertex.y;
        // o.debug = waveData.vertex;
        v.normal = waveData.normal;
        v.normal.z = 1-v.normal.z;
    }

此时的效果作用于水面模型顶点，可以通过Lod来使得离摄像机进的地方采用更加细致的高模来实现性能与表现之间的权衡。

未接入到当前URP项目里，偷一波网上的实现效果：https://blog.csdn.net/qq_43057783/article/details/102619509

## 水泡沫部分（思路
https://upload-images.jianshu.io/upload_images/19216342-8d91ef124d2dca66?imageMogr2/auto-orient/strip|imageView2/2/w/360/format/webp
看下水面与物体相交部分的白色部分。回到深度图，可以发现泡沫出现的部分就是深度图较小的部分。因此只需要再深度较浅的地方画上泡沫即可。同样的，现在泡沫部分都是由纯色组成，想要更加细致的效果，可以通过传入贴图来进行遮罩。


# ~~未涉及部分 水体交互~~


